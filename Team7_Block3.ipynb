{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomecek/bigdata_team7/blob/main/Team7_Block3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8-7lQ5jH0nq"
      },
      "outputs": [],
      "source": [
        "# installing jdk\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#downloading .tgz installation file for Spache spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "# installing apache spark from downloaded file\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "# installing findspark library\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importin necessary libraries fro this notebook\n",
        "import os\n",
        "import findspark\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StringType # Import StringType from pyspark.sql.types\n",
        "from pyspark.sql.window import Window # Import the Window class\n",
        "from pyspark.sql.functions import when, first, col # Import necessary functions\n",
        "\n"
      ],
      "metadata": {
        "id": "1n2WXRmMIFLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up paths for JDK and spark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\"\n",
        "#initiating findspark\n",
        "#findspark.init()\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "eUNf7NVwIJqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"SparkDayOne\").getOrCreate()"
      ],
      "metadata": {
        "id": "2Uvvwy_HIMmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.getActiveSession"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "iCxQKTBnIUWk",
        "outputId": "51a6a89f-b52d-4db7-8323-b133adfdecd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method SparkSession.getActiveSession of <class 'pyspark.sql.session.SparkSession'>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.session.SparkSession.getActiveSession</b><br/>def getActiveSession(cls) -&gt; Optional[&#x27;SparkSession&#x27;]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py</a>Returns the active :class:`SparkSession` for the current thread, returned by the builder\n",
              "\n",
              ".. versionadded:: 3.0.0\n",
              "\n",
              ".. versionchanged:: 3.5.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Returns\n",
              "-------\n",
              ":class:`SparkSession`\n",
              "    Spark session if an active session exists for the current thread\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; s = SparkSession.getActiveSession()\n",
              "&gt;&gt;&gt; df = s.createDataFrame([(&#x27;Alice&#x27;, 1)], [&#x27;name&#x27;, &#x27;age&#x27;])\n",
              "&gt;&gt;&gt; df.select(&quot;age&quot;).show()\n",
              "+---+\n",
              "|age|\n",
              "+---+\n",
              "|  1|\n",
              "+---+</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 646);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import train data"
      ],
      "metadata": {
        "id": "TrCSYrYXKcjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
        "df_train.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Y1ZzRk_bImed",
        "outputId": "d6a58260-e192-4fcf-bca1-b5411c3ddf19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/train.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-68b0b5c7c17b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/train.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning dle SSN\n",
        "\n",
        "prepsat_ssn_value = \"#F%$D@*&8\"\n",
        "colums_doplnit = [\"Customer_ID\", \"Name\", \"Occupation\", \"SSN\", \"Age\"]\n",
        "window_part = Window.partitionBy(\"Customer_ID\")\n",
        "\n",
        "df_filled = df_train.withColumn(\n",
        "    \"Name\", when(col(\"Name\").isNull(), first(\"Name\", True).over(window_part)).otherwise(col(\"Name\"))\n",
        ").withColumn(\n",
        "    \"Occupation\", when(col(\"Occupation\").isNull(), first(\"Occupation\", True).over(window_part)).otherwise(col(\"Occupation\"))\n",
        ").withColumn(\n",
        "    \"SSN\",\n",
        "    when((col(\"SSN\").isNull()) | (col(\"SSN\") == prepsat_ssn_value), first(when(col(\"SSN\") != prepsat_ssn_value, col(\"SSN\")), True).over(window_part)).otherwise(col(\"SSN\"))\n",
        ").withColumn(\n",
        "    \"Age\", when(col(\"Age\").isNull(), first(\"Age\", True).over(window_part)).otherwise(col(\"Age\"))\n",
        ")\n",
        "\n",
        "df_filled.show()"
      ],
      "metadata": {
        "id": "47IWMZ7IW3DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Show combinations of Customer_ID and SSN where single SSN is mapped to multiple different Customer_ID values\n",
        "\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Group by SSN and count the distinct Customer_IDs\n",
        "ssn_counts = df_filled.groupBy(\"SSN\").agg(count(\"Customer_ID\").alias(\"customer_count\"))\n",
        "\n",
        "# Filter for SSNs with more than one distinct Customer_ID\n",
        "multiple_customer_ids = ssn_counts.filter(\"customer_count > 1\")\n",
        "\n",
        "# Join with the original DataFrame to get the Customer_ID and SSN combinations\n",
        "result_df = multiple_customer_ids.join(df_filled, \"SSN\", \"inner\").select(\"Customer_ID\", \"SSN\")\n",
        "\n",
        "# Show the results\n",
        "result_df.show()"
      ],
      "metadata": {
        "id": "BSFdG542YFFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Find unique combinations of Customer_ID and SSN where a single SSN is mapped to multiple Customer_ID values\n",
        "\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "# Group by SSN and count the number of unique Customer_IDs\n",
        "ssn_counts = df_filled.groupBy(\"SSN\").agg(count(\"Customer_ID\").alias(\"customer_count\"))\n",
        "\n",
        "# Filter for SSNs with more than one Customer_ID\n",
        "multiple_customer_ssns = ssn_counts.filter(col(\"customer_count\") > 1)\n",
        "\n",
        "# Join with the original DataFrame to get the Customer_IDs associated with these SSNs\n",
        "result_df = multiple_customer_ssns.join(df_train, \"SSN\", \"inner\").select(\"Customer_ID\", \"SSN\")\n",
        "\n",
        "# Show the results\n",
        "result_df.orderBy(col(\"SSN\")).show()"
      ],
      "metadata": {
        "id": "MEXCDIUqWHkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean up underscores"
      ],
      "metadata": {
        "id": "haYkOBmJTbBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_underscore_columns(spark_df):\n",
        "\n",
        "    underscore_columns = []\n",
        "    for col in spark_df.columns:\n",
        "        # Check if column is of string type\n",
        "        if isinstance(spark_df.schema[col].dataType, (StringType)): # Use the imported StringType\n",
        "          # Convert the column to pandas series to enable string functions\n",
        "          pandas_series = spark_df.select(col).toPandas()[col]\n",
        "          if any('_' in str(x) for x in pandas_series):\n",
        "            underscore_columns.append(col)\n",
        "\n",
        "    return underscore_columns\n",
        "\n",
        "# Example usage (assuming df_train is your DataFrame)\n",
        "underscore_cols = find_underscore_columns(df_filled)\n",
        "\n",
        "# Remove 'Customer_ID' if present\n",
        "if 'Customer_ID' in underscore_cols:\n",
        "    underscore_cols.remove('Customer_ID')\n",
        "\n",
        "underscore_cols"
      ],
      "metadata": {
        "id": "EwppUOzTSPwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "def remove_underscores_from_columns(spark_df, cols_to_modify):\n",
        "    new_df = spark_df\n",
        "    for col in cols_to_modify:\n",
        "        new_df = new_df.withColumn(col, regexp_replace(col, \"_\", \"\"))\n",
        "    return new_df\n",
        "\n",
        "new_df_train = remove_underscores_from_columns(df_filled, underscore_cols)\n",
        "new_df_train.show()"
      ],
      "metadata": {
        "id": "i3hzUDXSRc6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean from underscores"
      ],
      "metadata": {
        "id": "9EMnV2cLQbx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics for individual columns"
      ],
      "metadata": {
        "id": "kCkKLg4zZKbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifikace numerických sloupců\n",
        "numerical_cols = [field.name for field in new_df_train.schema.fields if \"IntegerType\" in str(field.dataType) or \"DoubleType\" in str(field.dataType)]\n",
        "\n",
        "# Popis numerických sloupců\n",
        "for col_name in numerical_cols:\n",
        "    print(f\"Statistiky pro sloupec: {col_name}\")\n",
        "    new_df_train.select(col_name).describe().show()"
      ],
      "metadata": {
        "id": "Sfq9tkH1ZEyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify data types"
      ],
      "metadata": {
        "id": "7y9m83QFYXqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data Types:\")\n",
        "new_df_train.printSchema()"
      ],
      "metadata": {
        "id": "E7CrPGxNYZf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic statistics for numerical columns"
      ],
      "metadata": {
        "id": "U1fLSgcqYfjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df_train.describe().show()"
      ],
      "metadata": {
        "id": "f8mj-LMkYgmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find duplicates"
      ],
      "metadata": {
        "id": "XMvGfNa3Ld1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_duplicates = new_df_train.groupBy(df_train.columns).count().filter(\"count > 1\")\n",
        "df_duplicates.show()"
      ],
      "metadata": {
        "id": "m5_wYxN7Iu4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find missing values"
      ],
      "metadata": {
        "id": "slSaRr4dUhgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "missing_values = new_df_train.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_train.columns])\n",
        "missing_values.show()"
      ],
      "metadata": {
        "id": "-dCQUczdIw6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Seznam sloupců, které chceme převést na číselné hodnoty\n",
        "numeric_columns = [\"Age\", \"Annual_Income\", \"Monthly_Inhand_Salary\",\n",
        "                   \"Num_Bank_Accounts\", \"Num_Credit_Card\", \"Interest_Rate\",\n",
        "                   \"Num_of_Loan\", \"Num_of_Delayed_Payment\",\n",
        "                   \"Changed_Credit_Limit\", \"Num_Credit_Inquiries\",\n",
        "                   \"Outstanding_Debt\", \"Credit_Utilization_Ratio\",\n",
        "                   \"Credit_History_Age\", \"Total_EMI_per_month\",\n",
        "                   \"Amount_invested_monthly\", \"Monthly_Balance\"]\n",
        "\n",
        "# Převod každého sloupce na číselný typ s ošetřením chybějících nebo neplatných hodnot\n",
        "for column in numeric_columns:\n",
        "    data = data.withColumn(column, col(column).cast(\"double\"))\n",
        "\n",
        "data.show()"
      ],
      "metadata": {
        "id": "SZjmShZMLx3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data types after type casting"
      ],
      "metadata": {
        "id": "POKtOR5HbH2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "id": "7R9Hp74jbDpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Suggest 10 new features and add them to the \"data\" dataframe\n",
        "\n",
        "from pyspark.sql.functions import rand, when, lit\n",
        "\n",
        "# Assuming 'data' is your DataFrame (replace with the actual name if different)\n",
        "# and it's already defined in your existing code.\n",
        "# Example: data = new_df_train\n",
        "\n",
        "# 1. Debt to Income Ratio\n",
        "data = data.withColumn(\"Debt_to_Income_Ratio\", col(\"Outstanding_Debt\") / col(\"Annual_Income\"))\n",
        "\n",
        "# 2. Savings_Rate: Podíl mìsíèního zùstatku k mìsíènímu pøíjmu v procentech.\n",
        "# Ukazuje, jak efektivnì zákazník šetøí peníze z mìsíèního pøíjmu.\n",
        "data = data.withColumn(\n",
        "    \"Savings_Rate\",\n",
        "    when(col(\"Monthly_Inhand_Salary\") > 0, (col(\"Monthly_Balance\") / col(\"Monthly_Inhand_Salary\")))\n",
        "     .otherwise(None)\n",
        ")\n",
        "\n",
        "# 3. Total Delayed Days\n",
        "data = data.withColumn(\n",
        "    \"Total_Delayed_Days\",\n",
        "    (col(\"Num_of_Delayed_Payment\") * col(\"Delay_from_due_date\")).cast(\"float\")\n",
        ")\n",
        "\n",
        "# 4. EMI to Income Ratio\n",
        "data = data.withColumn(\n",
        "    \"EMI_to_Income_Ratio\",\n",
        "    (col(\"Total_EMI_per_month\") / col(\"Monthly_Inhand_Salary\")).cast(\"float\")\n",
        ")\n",
        "\n",
        "# 5. Loan Type Factor\n",
        "data = data.withColumn(\n",
        "    \"Loan_Type_Factor\",\n",
        "    when(col(\"Type_of_Loan\").like(\"%Home%\"), col(\"Num_of_Loan\") * 1.5)\n",
        "    .when(col(\"Type_of_Loan\").like(\"%Car%\"), col(\"Num_of_Loan\") * 1.2)\n",
        "    .otherwise(col(\"Num_of_Loan\"))\n",
        ")\n",
        "\n",
        "# 6. Financial Health Score\n",
        "data = data.withColumn(\n",
        "    \"Financial_Health_Score\",\n",
        "    (col(\"Annual_Income\") - col(\"Outstanding_Debt\") - (col(\"Total_EMI_per_month\") * 12)).cast(\"float\")\n",
        ")\n",
        "\n",
        "# 7. Income to Credit Card\n",
        "data = data.withColumn(\n",
        "    \"Income_Per_Credit_Card\",\n",
        "    (col(\"Annual_Income\") / col(\"Num_Credit_Card\")).cast(\"float\")\n",
        ")\n",
        "\n",
        "# 8. Delays Per Loan\n",
        "new_data = data.withColumn(\n",
        "    \"Delays_Per_Loan\",\n",
        "    (col(\"Num_of_Delayed_Payment\") / col(\"Num_of_Loan\")).cast(\"float\")\n",
        ")\n",
        "\n",
        "new_data.show()"
      ],
      "metadata": {
        "id": "rvspYQ4omB0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_features = [\n",
        "    \"Debt_to_Income_Ratio\",\n",
        "    \"Savings_Rate\",\n",
        "    \"Total_Delayed_Days\",\n",
        "    \"EMI_to_Income_Ratio\",\n",
        "    \"Loan_Type_Factor\",\n",
        "    \"Financial_Health_Score\",\n",
        "    \"Income_Per_Credit_Card\",\n",
        "    \"Delays_Per_Loan\"\n",
        "]"
      ],
      "metadata": {
        "id": "k3fhYcxUtXiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical analysis of new features"
      ],
      "metadata": {
        "id": "MTB5hEqRu1HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name in new_features:\n",
        "    print(f\"Description for column: {col_name}\")\n",
        "    new_data.select(col_name).describe().show()\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "IZWiL464t0ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data types of new features"
      ],
      "metadata": {
        "id": "pT1R5yLZu-cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name in new_features:\n",
        "    print(f\"Data type of column '{col_name}': {new_data.schema[col_name].dataType}\")"
      ],
      "metadata": {
        "id": "KL8lxCauuk-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to plot distributions\n",
        "def plot_feature_distributions(data, new_features, bins=30):\n",
        "    # Convert the PySpark DataFrame to Pandas before plotting\n",
        "    pandas_df = data.select(new_features).toPandas()\n",
        "    for feature in new_features:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(pandas_df[feature], kde=True, bins=bins, color='blue', alpha=0.6)\n",
        "        plt.title(f'Distribution of {feature}', fontsize=14)\n",
        "        plt.xlabel(feature, fontsize=12)\n",
        "        plt.ylabel('Frequency', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_feature_distributions(new_data, new_features)"
      ],
      "metadata": {
        "id": "Cfw-uOMKvOU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target leakage"
      ],
      "metadata": {
        "id": "SQuE8F7uzEH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = \"Credit_Score\"  # Replace with your target variable\n",
        "\n",
        "# 1. Correlation analysis (for numeric target)\n",
        "# Convert the column to a Pandas Series for type checking\n",
        "target_column_type = new_data.select(target_column).toPandas()[target_column].dtype\n",
        "\n",
        "# Check if the target column is numeric or object (string)\n",
        "if pd.api.types.is_numeric_dtype(target_column_type):\n",
        "    # Convert to Pandas DataFrame for correlation calculation\n",
        "    pandas_df = new_data[new_features + [target_column]].toPandas()\n",
        "    correlation_matrix = pandas_df.corr()\n",
        "    print(\"Correlation matrix:\\n\", correlation_matrix)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "else:  # Handle categorical target variable\n",
        "    # Convert 'Credit_Score' to numerical representation\n",
        "    from pyspark.sql.functions import when\n",
        "\n",
        "    new_data = new_data.withColumn(\n",
        "        target_column,\n",
        "        when(col(target_column) == \"Good\", 1)\n",
        "        .when(col(target_column) == \"Standard\", 0)\n",
        "        .otherwise(None)  # Handle other categories if needed\n",
        "    )\n",
        "    # Now you can proceed with the correlation analysis using the updated 'new_data'\n",
        "    pandas_df = new_data[new_features + [target_column]].toPandas()\n",
        "    correlation_matrix = pandas_df.corr()\n",
        "    print(\"Correlation matrix:\\n\", correlation_matrix)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 2. Distribution across target categories (for categorical target)\n",
        "# ... (rest of your code remains the same)"
      ],
      "metadata": {
        "id": "JxGXBidruyM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = [\n",
        "    \"Age\", \"Annual_Income\", \"Monthly_Inhand_Salary\",\n",
        "                   \"Num_Bank_Accounts\", \"Num_Credit_Card\", \"Interest_Rate\",\n",
        "                   \"Num_of_Loan\", \"Num_of_Delayed_Payment\",\n",
        "                   \"Changed_Credit_Limit\", \"Num_Credit_Inquiries\",\n",
        "                   \"Outstanding_Debt\", \"Credit_Utilization_Ratio\",\n",
        "                   \"Credit_History_Age\", \"Total_EMI_per_month\",\n",
        "                   \"Amount_invested_monthly\", \"Monthly_Balance\",\n",
        "    \"Debt_to_Income_Ratio\",\n",
        "    \"Savings_Rate\",\n",
        "    \"Total_Delayed_Days\",\n",
        "    \"EMI_to_Income_Ratio\",\n",
        "    \"Loan_Type_Factor\",\n",
        "    \"Financial_Health_Score\",\n",
        "    \"Income_Per_Credit_Card\",\n",
        "    \"Delays_Per_Loan\"\n",
        "]"
      ],
      "metadata": {
        "id": "-sTq-MGTckNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = \"Credit_Score\"  # Replace with your target variable\n",
        "\n",
        "# 1. Correlation analysis (for numeric target)\n",
        "# Convert the column to a Pandas Series for type checking\n",
        "target_column_type = new_data.select(target_column).toPandas()[target_column].dtype\n",
        "\n",
        "# Check if the target column is numeric or object (string)\n",
        "if pd.api.types.is_numeric_dtype(target_column_type):\n",
        "    # Convert to Pandas DataFrame for correlation calculation\n",
        "    pandas_df = new_data[all_features + [target_column]].toPandas()\n",
        "    correlation_matrix = pandas_df.corr()\n",
        "    print(\"Correlation matrix:\\n\", correlation_matrix)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "else:  # Handle categorical target variable\n",
        "    # Convert 'Credit_Score' to numerical representation\n",
        "    from pyspark.sql.functions import when\n",
        "\n",
        "    new_data = new_data.withColumn(\n",
        "        target_column,\n",
        "        when(col(target_column) == \"Good\", 1)\n",
        "        .when(col(target_column) == \"Standard\", 0)\n",
        "        .otherwise(None)  # Handle other categories if needed\n",
        "    )\n",
        "    # Now you can proceed with the correlation analysis using the updated 'new_data'\n",
        "    pandas_df = new_data[all_features + [target_column]].toPandas()\n",
        "    correlation_matrix = pandas_df.corr()\n",
        "    print(\"Correlation matrix:\\n\", correlation_matrix)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 2. Distribution across target categories (for categorical target)\n",
        "# ... (rest of your code remains the same)"
      ],
      "metadata": {
        "id": "f7qS-7EWcy9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zmena - test\n"
      ],
      "metadata": {
        "id": "AuUTrz2ClYi0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcPS8Ja4lapz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}